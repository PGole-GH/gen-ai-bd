{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1bbe854-3937-4875-9152-87709aac43a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SETUP ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8b0612d-71f9-4811-869f-b9d8afe64c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[676, 461, 7, 753, 699]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark \n",
    "sc = pyspark.SparkContext('local[*]')\n",
    "# do something to prove it works\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882eb327-a85f-4299-b7a2-d3da4e4eaad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Read CSV in Jupyter with PySpark\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f84565c9-6f66-4908-8c59-bedb861cbb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500000\n"
     ]
    }
   ],
   "source": [
    "accounts = spark.read.csv(\"files/accounts.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "print(accounts.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a01d24-0244-45e6-87ab-55a3df4451f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    }
   ],
   "source": [
    "country_abbrev = spark.read.csv(\"files/country_abbreviation.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "print(country_abbrev.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ab35e91-d10c-4da7-9096-38e3f5385177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000000\n"
     ]
    }
   ],
   "source": [
    "transactions = spark.read.csv(\"files/transactions.csv\", header=True, inferSchema=True, sep=\";\")\n",
    "print(transactions.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0890076-37d1-4fbc-b395-17f2ea3be15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 1 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b0d6bab-7881-4397-ba7d-372bd81ff46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum, max, col, round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b6ee3b7-d70e-4935-b4a5-f92b5ca2788d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, amount: double, account_type: string, transaction_date: date, country: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(transactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6de9199-cc82-4abf-86d6-7d7f1e7feb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|account_type|account_type_count|\n",
      "+------------+------------------+\n",
      "|    Personal|           1667072|\n",
      "|Professional|           1667358|\n",
      "|    Business|           1665570|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "account_counts = transactions.groupBy(\"account_type\").agg(count(\"id\").alias(\"account_type_count\"))\n",
    "account_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "440c7300-998b-416c-979f-3a4811e75379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-----------+\n",
      "|    id| balance|latest_date|\n",
      "+------+--------+-----------+\n",
      "|482333|27174.07| 2020-07-17|\n",
      "|222048|48004.81| 2020-07-20|\n",
      "|328078|36948.25| 2020-02-01|\n",
      "|192401|36736.98| 2020-01-30|\n",
      "|273916|47475.38| 2021-05-30|\n",
      "|485103|62198.93| 2021-05-22|\n",
      "|300282|55103.62| 2021-05-01|\n",
      "| 20683|56448.72| 2021-10-27|\n",
      "| 15846|58671.91| 2020-12-23|\n",
      "|446783|98085.51| 2021-12-11|\n",
      "| 92182| 42335.3| 2020-08-08|\n",
      "|477485|22114.03| 2020-05-23|\n",
      "|171142| 40428.9| 2021-04-07|\n",
      "|317762|40025.55| 2021-12-02|\n",
      "| 65478| 57941.9| 2021-10-06|\n",
      "|306768|26566.93| 2019-12-19|\n",
      "|380411|43652.94| 2020-06-02|\n",
      "|304681|37827.69| 2021-03-26|\n",
      "|475638| 44509.1| 2021-11-23|\n",
      "| 97413|39611.24| 2018-05-01|\n",
      "+------+--------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "balance_and_date = transactions.groupBy(\"id\")\\\n",
    "    .agg(\n",
    "        round(sum(\"amount\"), 2).alias(\"balance\"),\n",
    "        max(\"transaction_date\").alias(\"latest_date\")\n",
    "    )\\\n",
    "    .withColumn(\"balance\", col(\"balance\").cast(\"string\"))\n",
    "\n",
    "balance_and_date.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe357214-8d14-463f-a241-85a773fca7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 2 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3eb72c4-b2ab-4355-bf85-300b99f2a8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, concat_ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ed3e691-6f5e-4783-a9cf-27af938cfaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def earnings_pivot_table(accounts, country_abbreviation, transactions):\n",
    "    # Filter transactions with earnings\n",
    "    earnings_df = transactions.filter(transactions.amount > 0)\n",
    "    \n",
    "    # Alias the DataFrames\n",
    "    acc_alias = accounts.alias(\"acc\")\n",
    "    country_alias = country_abbreviation.alias(\"country\")\n",
    "    trans_alias = earnings_df.alias(\"trans\")\n",
    "    \n",
    "    # Join transactions with accounts\n",
    "    joined_df = trans_alias.join(acc_alias, trans_alias.id == acc_alias.id, 'inner').select(trans_alias[\"*\"], acc_alias[\"first_name\"], acc_alias[\"last_name\"])\n",
    "    \n",
    "    # Join with country abbreviations and filter for Switzerland\n",
    "    swiss_earnings_df = joined_df.join(country_alias, joined_df.country == country_alias.abbreviation).filter(country_alias.country_full_name == 'Switzerland')\n",
    "    \n",
    "    # Extract year from transaction_date\n",
    "    swiss_earnings_df = swiss_earnings_df.withColumn(\"year\", year(swiss_earnings_df.transaction_date))\n",
    "    \n",
    "    # Group by full name and year\n",
    "    grouped_df = swiss_earnings_df.groupBy(concat_ws(' ', swiss_earnings_df.first_name, swiss_earnings_df.last_name).alias(\"full_name\"), \"year\").agg(round(sum(\"amount\"), 2).alias(\"earnings\"))\n",
    "    \n",
    "    # Pivot table\n",
    "    pivot_df = grouped_df.groupBy(\"full_name\").pivot(\"year\").sum(\"earnings\").fillna(0)\n",
    "\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "766ee7e9-456f-458f-9a13-d3fa72bb697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+-------+-------+--------+-------+-------+--------+-------+-------+-------+-------+\n",
      "|         full_name|   2011|   2012|   2013|    2014|   2015|   2016|    2017|   2018|   2019|   2020|   2021|\n",
      "+------------------+-------+-------+-------+--------+-------+-------+--------+-------+-------+-------+-------+\n",
      "|     Lenny Spencer|2050.35|    0.0|    0.0|     0.0|    0.0| 509.08|16116.58|    0.0|8693.52| 923.65|8797.15|\n",
      "|      Lucia Watson|    0.0|2173.26|    0.0|10047.25|    0.0|    0.0|     0.0|    0.0|4500.61|    0.0|    0.0|\n",
      "|      Jessica West|    0.0|8673.72|    0.0|     0.0|    0.0|    0.0|     0.0|    0.0|    0.0|4797.03|1294.49|\n",
      "|     Aston Andrews|    0.0|    0.0|    0.0|     0.0|    0.0|    0.0| 4598.25|    0.0|    0.0|    0.0|    0.0|\n",
      "|   Kirsten Stevens| 8932.7|    0.0|5753.21|     0.0|    0.0|3134.12|     0.0|    0.0|    0.0|    0.0|    0.0|\n",
      "|       Luke Carter|    0.0|1585.41|  93.69|     0.0|    0.0|    0.0| 7029.37|8340.16|    0.0|    0.0|    0.0|\n",
      "|     Darcy Edwards|    0.0|    0.0|    0.0|     0.0|    0.0|    0.0| 7892.65|8538.91|2252.85|    0.0|    0.0|\n",
      "|Jessica Richardson|    0.0|    0.0|    0.0|     0.0|    0.0|9661.02|  3273.5|    0.0|    0.0|    0.0|    0.0|\n",
      "|      Natalie Hill|    0.0|4019.35|    0.0| 3966.58|    0.0| 9315.2|     0.0|    0.0|    0.0|2789.47|    0.0|\n",
      "|       Agata Myers|    0.0|    0.0|    0.0|     0.0|    0.0|    0.0|     0.0|    0.0|    0.0|7926.86|    0.0|\n",
      "|      Arnold Kelly|    0.0|    0.0|    0.0|     0.0|7327.47|4848.64|     0.0|    0.0|    0.0|    0.0|    0.0|\n",
      "|     Sienna Watson|    0.0|    0.0|5427.76| 5458.24|    0.0|9114.71| 4854.05|    0.0|5794.61|    0.0|    0.0|\n",
      "|     Fenton Thomas|    0.0|1182.12|    0.0|     0.0|    0.0|    0.0|     0.0|    0.0|    0.0|    0.0|    0.0|\n",
      "|       Julia Ellis|    0.0|    0.0|    0.0| 6054.68|5205.99|8650.32|     0.0|    0.0|    0.0|    0.0|    0.0|\n",
      "|    Daisy Campbell|    0.0|6356.06|4818.39|     0.0|    0.0|    0.0|  156.42|    0.0|    0.0|    0.0|    0.0|\n",
      "|      Paige Taylor|9149.33|    0.0|    0.0|     0.0|8396.53|    0.0|     0.0|8368.08|    0.0|4075.26|7902.95|\n",
      "|        Lucia West|    0.0|    0.0|9872.33|     0.0|    0.0|    0.0| 1228.87|    0.0|    0.0|    0.0|    0.0|\n",
      "|        Myra Owens|7290.28|    0.0|    0.0| 7508.42|    0.0|    0.0| 19543.1|    0.0|    0.0|    0.0|    0.0|\n",
      "|        Carl Craig|    0.0|7126.05|    0.0|     0.0|    0.0|    0.0|     0.0|    0.0|9610.96| 920.41|    0.0|\n",
      "|      Camila Adams|    0.0|    0.0|5689.23|     0.0|    0.0|    0.0|     0.0|    0.0|    0.0|    0.0|    0.0|\n",
      "+------------------+-------+-------+-------+--------+-------+-------+--------+-------+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assuming dataframes are read from CSVs\n",
    "result_df = earnings_pivot_table(accounts, country_abbrev, transactions)\n",
    "\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6c1463a-852e-41a3-834f-7ea13e75a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 3 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2a3a1a6-7a6e-4d33-8813-65d02ab3a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d099bf4-e41b-4996-9ae5-26af455067d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_transaction_level(transactions_df):\n",
    "    # Create a window spec\n",
    "    windowSpec = Window.orderBy(F.desc(\"amount\"))\n",
    "\n",
    "    # Calculate percent rank\n",
    "    transactions_df = transactions_df.withColumn(\"percent_rank\", F.percent_rank().over(windowSpec))\n",
    "\n",
    "    # Define \"level\" based on the percentile rank\n",
    "    transactions_df = transactions_df.withColumn(\n",
    "        \"level\",\n",
    "        F.when(transactions_df[\"percent_rank\"] <= 0.25, \"high\")\n",
    "        .when((transactions_df[\"percent_rank\"] > 0.25) & (transactions_df[\"percent_rank\"] <= 0.75), \"average\")\n",
    "        .otherwise(\"low\")\n",
    "    )\n",
    "\n",
    "    # Drop the temporary percent_rank column\n",
    "    transactions_df = transactions_df.drop(\"percent_rank\")\n",
    "\n",
    "    return transactions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa1b120d-7737-4180-bf74-e1407cdd0676",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASK 4 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "20742837-47ac-40ec-92ac-2a397c5fe653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02a2e610-2378-4ca4-a776-4da35dcaae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_custom_format_spark(file_path):\n",
    "    # Read lines from file\n",
    "    rdd = spark.sparkContext.textFile(file_path)\n",
    "\n",
    "    # Filter out separator lines\n",
    "    lines = rdd.filter(lambda line: not line.startswith('+'))\n",
    "\n",
    "    # Extract column names from the first valid line\n",
    "    header = lines.first()\n",
    "    columns = header.split('|')\n",
    "    columns = [col.strip() for col in columns if col]\n",
    "\n",
    "    # Extract rows\n",
    "    def parse_line(line):\n",
    "        items = line.split('|')\n",
    "        items = [item.strip() for item in items if item]\n",
    "        return Row(**{columns[i]: items[i] for i in range(len(columns))})\n",
    "\n",
    "    rows = lines.filter(lambda line: line != header).map(parse_line)  # Use the cached header\n",
    "\n",
    "    # Convert RDD to DataFrame\n",
    "    df = spark.createDataFrame(rows)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eef5c126-1454-48bf-85ba-3240476c12e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, Col1: string, Col2: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage:\n",
    "file_path = \"files/test.txt\"\n",
    "df = read_custom_format_spark(file_path)\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
